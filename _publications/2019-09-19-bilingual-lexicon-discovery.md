---
title: "Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio"
collection: publications
permalink: /publication/2019-09-19-bilingual-lexicon-discovery
excerpt: 'This work presented a multimodal approach to learn bilingual lexicon directly from speech signals in two languages without the need for text by using vision as an interlingua. The approach starts a line of inquiry that can build word level translation between a pair of languages using say youtube videos that have similar objects but with speech in the two languages.'
date: 2019-09-19
venue: 'Interspeech'
paperurl: 'http://groups.csail.mit.edu/sls/publications/2019/EmmanuelAzuh_Interspeech-2019.PDF'
citation: 'Azuh, Emmanuel, David Harwath, and James R. Glass. "Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio." INTERSPEECH. 2019.'
---
This work presented a multimodal approach to learn bilingual lexicon directly from speech signals in two languages without the need for text by using vision as an interlingua. The approach starts a line of inquiry that can build word level translation between a pair of languages using say youtube videos that have similar objects but with speech in the two languages.

# [Download paper here](http://groups.csail.mit.edu/sls/publications/2019/EmmanuelAzuh_Interspeech-2019.PDF)

Recommended citation: Azuh, Emmanuel, David Harwath, and James R. Glass. "Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio." INTERSPEECH. 2019.
